---
title: "Practical Machine Learning project"
author: "Pablo Garcia"
date: "October 23, 2015"
output: html_document
---

# Introduction

This report illlustrates the use of a ramdom forest algorithm to predict correct or incorrect weight lifting exercise. Detailed information about the original research and dataset cona be found in the following link [Weight Lifting]( http://groupware.les.inf.puc-rio.br/har).

# Data loading and cleaning

Data sets includes two different sets.  
The [training set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) will be used for both training and crossvalidation. The data will be split in two different sunsets called train and crossValid.  
The [test set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) will be used to compute 20 predictions that will be submitted for automatic grading.

First both datasets are loaded.
```{r}
trainSource <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training <-read.csv(trainSource,
                    stringsAsFactors = FALSE)

testSource <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testing <-read.csv(testSource,
                    stringsAsFactors = FALSE)

```

No feature are inspected in order to remove ones with missing values. In addition as the two datsets contain different number of features, names are intersected in order to select common features, finally classe features is add to training set again as this is the output varible and hence is not included in the test set.


```{r}

# build a logical vector of missing values or not for each set.
misTrain <- apply(training, 2, function(e) {sum(is.na(e)) == 0})
misTest <- apply(testing, 2, function(e) {sum(is.na(e)) == 0})

par(mfrow = c(2,1))
plot(misTrain, main = "Training set. Feature without missing values",
     xlab = "Features", ylab = "", yaxt = "n")
axis(2, at = c(0,1), labels = c("FALSE", "TRUE"))
plot(misTest, main = "Testing set. Feature without missing values",
     xlab = "Features", ylab = "", yaxt = "n")
axis(2, at = c(0,1), labels = c("FALSE", "TRUE"))

# intersect what possible varibles are in both datasets
trainingPossFeat <- which(misTrain == TRUE)
testingPossFeat <- which(misTest == TRUE)

names(training)[trainingPossFeat]
names(testing)[testingPossFeat]

training <- training[ ,trainingPossFeat]
testing <- testing[ ,testingPossFeat]

trainInTestFeat <- intersect(names(training), names(testing))

training <- training[ ,c(trainInTestFeat, "classe")]
testing <- testing[, c(trainInTestFeat)]

training <- training[ , -c(1:7)]
testing <- testing[ , -c(1:7)]

```

# Training set split and outliers removal.

Training set is splitted into train set that will be used to build the model and a cross validation set to estimate out of sample error.

Based on visual inspection some outliers are removed from data set

```{r}
set.seed(1234)
library(caret)
inTrain <- createDataPartition(training$classe, p = 0.7, list = F)

train <- training[ inTrain, ]
test <- training[- inTrain, ]

par( mfrow = c(1,3))

  plot(as.factor(train[ ,53]), train[ ,31], 
       main = paste("feature ", names(train)[31]))
  plot(as.factor(train[ ,53]), train[ ,32], 
       main = paste("feature ", names(train)[32]))
  plot(as.factor(train[ ,53]), train[ ,44], 
       main = paste("feature ", names(train)[44]))
  
# remove outliers
train <- train[-c(700,3759,9647), ]


par( mfrow = c(1,3))

  plot(as.factor(train[ ,53]), train[ ,31], 
       main = paste("feature ", names(train)[31]))
  plot(as.factor(train[ ,53]), train[ ,32], 
       main = paste("feature ", names(train)[32]))
  plot(as.factor(train[ ,53]), train[ ,44], 
       main = paste("feature ", names(train)[44]))

```

# Preprocessing and cross validation

A random forest algorithm is used to compute requested predictions. 

First preprocessing is performed using Principal compoent analysis. The percentage of variance retained is set at 95%. This reduce the number of features from 52 to 26.

No a cross validation function is apllied to preprocessed data set in order to evaluate if retain values are significant in terms of reducing out of sample error.


```{r}
library(randomForest)

  
pcaPrep <- preProcess(train[ ,-53],  method = "pca", thresh = .95)
trainPca <- predict(pcaPrep, train[ ,-53])

cvResult <- rfcv(trainPca, factor(train$classe))  

par( mfrow = c(1,1))
with(cvResult, plot(n.var, error.cv, 
                    main = "Cross Validation error", 
                    log="x", type="o", lwd=2))

```
Cross validation error using all is:

error.rate = `r format(cvResult$error.cv[1], digits = 3)`


Finally out of sample error is computed using test set data.

```{r}

testPca <- predict(pcaPrep, test[ ,-53])

pcaRf <- randomForest(as.factor(train$classe) ~ ., data = trainPca,
                     xtest = testPca, ytest = as.factor(test$classe))

```

error.rate = `r format( mean(pcaRf$test$err.rate[ ,1]) , digits = 3)`







testPred <- predict(pcaRf,newdata=testPca)

confusionMatrix(cv$classe, cvPred)
```








