---
title: "Practical Machine Learning project"
author: "Pablo Garcia"
date: "October 23, 2015"
output: html_document
---

# Introduction

This report illlustrates the use of a ramdom forest algorithm to predict correct or incorrect weight lifting exercises. Detailed information about original research and dataset can be found in the following link [Weight Lifting]( http://groupware.les.inf.puc-rio.br/har).

# Data loading and cleaning

Data sets includes two different sets.  
The [training set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) will be used for both training and crossvalidation. The data will be split into two different subsets named train and test.  
The [test set](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) will be used to compute 20 predictions that will be submitted for automatic grading, testing dataset is used during data cleaning so both datasets must include same future in order to compute predictions for automatic grading.

First both datasets are loaded.
```{r}
trainSource <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training <-read.csv(trainSource,
                    stringsAsFactors = FALSE)

testSource <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testing <-read.csv(testSource,
                    stringsAsFactors = FALSE)

```

Features are inspected in order to remove ones containing missing values. In addition as the datasets contain different number of features, names are intersected in order to select common features, finally classe features is add to training set again as this is the output varible and hence is not included in the testing set.


```{r, fig.height= 8}

# build a logical vector of missing values or not for each set.
misTrain <- apply(training, 2, function(e) {sum(is.na(e)) == 0})
misTest <- apply(testing, 2, function(e) {sum(is.na(e)) == 0})

par(mfrow = c(2,1))
plot(misTrain, main = "Training set. Feature without missing values",
     xlab = "Features", ylab = "", yaxt = "n")
axis(2, at = c(0,1), labels = c("FALSE", "TRUE"))
plot(misTest, main = "Testing set. Feature without missing values",
     xlab = "Features", ylab = "", yaxt = "n")
axis(2, at = c(0,1), labels = c("FALSE", "TRUE"))

# intersect what possible varibles are in both datasets
trainingPossFeat <- which(misTrain == TRUE)
testingPossFeat <- which(misTest == TRUE)


training <- training[ ,trainingPossFeat]
testing <- testing[ ,testingPossFeat]

trainInTestFeat <- intersect(names(training), names(testing))

training <- training[ ,c(trainInTestFeat, "classe")]
testing <- testing[, c(trainInTestFeat)]

training <- training[ , -c(1:7)]
testing <- testing[ , -c(1:7)]

```

# Training set split and outliers removal.

Training set is split into train set that will be used to build the model and a test set that will be used to estimate out of sample error.

Based on visual inspection some outliers are removed from data set

```{r}
set.seed(1234)
library(caret)
inTrain <- createDataPartition(training$classe, p = 0.7, list = F)


train <- training[ inTrain, ]
test <- training[- inTrain, ]

```

# Preprocessing and cross validation

A random forest algorithm is used to compute requested predictions. 

First preprocessing is performed using Principal Component Analysis. The percentage of variance retained is set at 95%. This reduce the number of features from 52 to 26.

Now cross validation function is apllied to preprocessed data set in order to evaluate if all features computed through pca are significant in terms of reducing out of sample error.


```{r}
library(randomForest)

pcaPrep <- preProcess(train[ ,-53],  method = "pca", thresh = .95)
trainPca <- predict(pcaPrep, train[ ,-53])

cvResult <- rfcv(trainPca, factor(train$classe))  

par( mfrow = c(1,1))
with(cvResult, plot(n.var, error.cv, 
                    main = "Cross Validation error", 
                    log="x", type="o", lwd=2))

```

Cross validation error using all is:

error.rate = `r format(cvResult$error.cv[1], digits = 3)`


```{r}

testPca <- predict(pcaPrep, test[ ,-53])

pcaRf <- randomForest(as.factor(train$classe) ~ ., data = trainPca,
                     xtest = testPca, ytest = as.factor(test$classe))

```

Finally out of sample error is computed using test set data.


error.rate = `r format( mean(pcaRf$test$err.rate[ ,1]) , digits = 3)`


Finally a summary of the results of the prediction on test set is included.

```{r}



confusionMatrix(as.factor(test$classe), pcaRf$test$predicted )

```







